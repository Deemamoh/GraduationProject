{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "588c9696-4278-404a-959d-06f8dcb70630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing libraries\n",
    "import pandas as pd  #Data manipulation\n",
    "import string  #Remove punctuation & characters\n",
    "import nltk  #Natural language processing \n",
    "import pickle  #For loading saved models and vectorizers\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords  #Stop word removal\n",
    "from nltk.tokenize import word_tokenize  #Tokenizition\n",
    "#from nltk.stem import PorterStemmer  #Stemming\n",
    "from nltk.stem import WordNetLemmatizer  #Import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet  #Import WordNet\n",
    "\n",
    "#Feature extractions libraries\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#Models libraries\n",
    "from sklearn.model_selection import train_test_split #For data splitting\n",
    "#Model Evaluation Function\n",
    "from sklearn.metrics import accuracy_score, classification_report  #Import metrics\n",
    "from sklearn.svm import SVC  #SVM Model\n",
    "from sklearn.naive_bayes import MultinomialNB #Naive Bayes Model -  MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9f93e7c3-5464-4500-888d-ed7453585d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read excel file\n",
    "file_path = r\"C:\\Users\\HUAWEI\\Downloads\\bbc-text 2227 - Copy.csv\"\n",
    "data = pd.read_csv(file_path, delimiter=';')\n",
    "text = data['text']  #get text column\n",
    "category = data['category']  #get category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2357c21a-5fff-4a69-8efc-d08e9831e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [tv, future, hand, viewer, home, theatre, syst...\n",
      "1       [worldcom, bos, leave, book, alone, former, wo...\n",
      "2       [tiger, wary, farrell, gamble, leicester, say,...\n",
      "3       [yeading, face, newcastle, fa, cup, premiershi...\n",
      "4       [ocean, twelve, raid, box, office, ocean, twel...\n",
      "                              ...                        \n",
      "2220    [car, pull, u, retail, figure, u, retail, sale...\n",
      "2221    [kilroy, unveils, immigration, policy, exchats...\n",
      "2222    [rem, announce, new, glasgow, concert, u, band...\n",
      "2223    [political, squabble, snowball, become, common...\n",
      "2224    [souness, delight, euro, progress, bos, graeme...\n",
      "Name: lemmatized, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Data pre-processing\n",
    "#Normalization\n",
    "#Remove unwanted characters\n",
    "text = text.str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "#convert text column into lower case \n",
    "text = text.str.lower()\n",
    "\n",
    "#Tokenization\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "#Apply the tokenization function to the text column\n",
    "text['tokens'] = text.apply(tokenize_text) #Store the tokens in a new column\n",
    "tokens = text['tokens']\n",
    "\n",
    "#Define stop words for English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Stop word removal\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "#Apply the function to the tokens column\n",
    "tokens = tokens.apply(remove_stop_words)\n",
    "tokens\n",
    "\n",
    "#Initialize the WordNet lemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "#Function to map Part-of-speech (POS) tags to WordNet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ  #Adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB  #Verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN  #Noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV  #Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  #Default to noun\n",
    "\n",
    "#Define a function for lemmatizing words\n",
    "def lemmatizing(tokens):  #Accept a list of tokens\n",
    "    pos_tag = nltk.pos_tag(tokens)  #Get POS tags for the tokens\n",
    "    return [wn.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tag]  #Return lemmatized words\n",
    "\n",
    "\n",
    "data['lemmatized'] = tokens.apply(lemmatizing)  #Store the lemmatized tokens in a new column\n",
    "\n",
    "print(data['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2a1c613b-4165-4e7e-a440-43f045f78369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>russia gets investment blessing soaring oil sa...</td>\n",
       "      <td>[russia, get, investment, blessing, soar, oil,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>intel unveils laser breakthrough intel has sai...</td>\n",
       "      <td>[intel, unveils, laser, breakthrough, intel, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>holmes is hit by hamstring injury kelly holmes...</td>\n",
       "      <td>[holmes, hit, hamstring, injury, kelly, holmes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>blair labour s longest-serving pm tony blair h...</td>\n",
       "      <td>[blair, labour, longestserving, pm, tony, blai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>boateng to step down at election paul boateng ...</td>\n",
       "      <td>[boateng, step, election, paul, boateng, chief...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "917   russia gets investment blessing soaring oil sa...   \n",
       "1150  intel unveils laser breakthrough intel has sai...   \n",
       "395   holmes is hit by hamstring injury kelly holmes...   \n",
       "412   blair labour s longest-serving pm tony blair h...   \n",
       "1450  boateng to step down at election paul boateng ...   \n",
       "\n",
       "                                             lemmatized  \n",
       "917   [russia, get, investment, blessing, soar, oil,...  \n",
       "1150  [intel, unveils, laser, breakthrough, intel, s...  \n",
       "395   [holmes, hit, hamstring, injury, kelly, holmes...  \n",
       "412   [blair, labour, longestserving, pm, tony, blai...  \n",
       "1450  [boateng, step, election, paul, boateng, chief...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['text', 'lemmatized']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "37e6ad6a-a342-4a93-83a5-b5e7527688fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End of 1st step (data cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ac4d7be2-81a2-43cf-953d-0084c1df73f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Matrix:\n",
      "      00  000  000m  000strong  01  02  03  04  05  06  ...  zach  zealand  \\\n",
      "0      0    1     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "1      0    1     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "2      0    0     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "3      0    0     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "4      0    0     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "...   ..  ...   ...        ...  ..  ..  ..  ..  ..  ..  ...   ...      ...   \n",
      "2220   0    0     0          0   1   0   4   0   1   2  ...     0        0   \n",
      "2221   0    2     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "2222   0    1     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "2223   0    1     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "2224   0    0     0          0   0   0   0   0   0   0  ...     0        0   \n",
      "\n",
      "      zeppelin  zero  zhang  zimbabwe  zombie  zone  zoom  zurich  \n",
      "0            0     0      0         0       0     0     0       0  \n",
      "1            0     0      0         0       0     0     0       0  \n",
      "2            0     0      0         0       0     0     0       0  \n",
      "3            0     0      0         0       0     0     0       0  \n",
      "4            0     0      0         0       0     0     0       0  \n",
      "...        ...   ...    ...       ...     ...   ...   ...     ...  \n",
      "2220         0     0      0         0       0     0     0       0  \n",
      "2221         0     0      0         0       0     0     0       0  \n",
      "2222         0     0      0         0       0     0     0       0  \n",
      "2223         0     0      0         0       0     0     0       0  \n",
      "2224         0     0      0         0       0     0     0       0  \n",
      "\n",
      "[2225 rows x 7597 columns]\n",
      "\n",
      "TF-IDF Matrix:\n",
      "       00       000  000m  000strong        01   02        03   04        05  \\\n",
      "0     0.0  0.021520   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "1     0.0  0.024853   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "2     0.0  0.000000   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "3     0.0  0.000000   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "4     0.0  0.000000   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "...   ...       ...   ...        ...       ...  ...       ...  ...       ...   \n",
      "2220  0.0  0.000000   0.0        0.0  0.063294  0.0  0.257846  0.0  0.065767   \n",
      "2221  0.0  0.096262   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "2222  0.0  0.028430   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "2223  0.0  0.018027   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "2224  0.0  0.000000   0.0        0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
      "\n",
      "            06  ...  zach  zealand  zeppelin  zero  zhang  zimbabwe  zombie  \\\n",
      "0     0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "1     0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "2     0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "3     0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "4     0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "...        ...  ...   ...      ...       ...   ...    ...       ...     ...   \n",
      "2220  0.144284  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "2221  0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "2222  0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "2223  0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "2224  0.000000  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   \n",
      "\n",
      "      zone  zoom  zurich  \n",
      "0      0.0   0.0     0.0  \n",
      "1      0.0   0.0     0.0  \n",
      "2      0.0   0.0     0.0  \n",
      "3      0.0   0.0     0.0  \n",
      "4      0.0   0.0     0.0  \n",
      "...    ...   ...     ...  \n",
      "2220   0.0   0.0     0.0  \n",
      "2221   0.0   0.0     0.0  \n",
      "2222   0.0   0.0     0.0  \n",
      "2223   0.0   0.0     0.0  \n",
      "2224   0.0   0.0     0.0  \n",
      "\n",
      "[2225 rows x 7597 columns]\n"
     ]
    }
   ],
   "source": [
    "#Try sparate Extractors - Algorithms\n",
    "#Convert lemmatized lists to strings for vectorization\n",
    "data['lemmatized_text'] = data['lemmatized'].apply(lambda x: ' '.join(x)) #To convert the texts in the “lemmatized” column into single texts by joining the words using spaces.\n",
    "\n",
    "#Bag of Words (BoW)\n",
    "bow_vectorizer = CountVectorizer(min_df=5) #to ignore words that appear in less than 5 titles.\n",
    "bow_matrix = bow_vectorizer.fit_transform(data['lemmatized_text']) #bow_matrix - contains the number of occurrences of each word.\n",
    "#Fit_transform - will be applied to the transformed texts to form a matrix.\n",
    "\n",
    "#Convert to DataFrame to display the data more clearly\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words (BoW) Matrix:\")\n",
    "print(bow_df)\n",
    "\n",
    "#TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5)#Ignore rare words\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['lemmatized_text'])#tfidf_matrix-Represents the weight of each word based on its frequency.\n",
    "#Fit_transform-To convert texts to a matrix.\n",
    "\n",
    "#Convert to DataFrame to display the data more clearly\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "704b1f25-2b64-437b-b557-84a2580b17bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9730337078651685\n",
      "SVM Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.98      0.92      0.95       101\n",
      "entertainment       0.98      0.99      0.98        81\n",
      "     politics       0.94      0.99      0.96        83\n",
      "        sport       0.98      1.00      0.99        98\n",
      "         tech       0.99      0.98      0.98        82\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n",
      "Naive Bayes Accuracy: 0.9662921348314607\n",
      "Naive Bayes Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.96      0.96       101\n",
      "entertainment       1.00      0.90      0.95        81\n",
      "     politics       0.93      0.98      0.95        83\n",
      "        sport       0.99      1.00      0.99        98\n",
      "         tech       0.95      0.99      0.97        82\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n",
      "Random Forest Accuracy: 0.952808988764045\n",
      "Random Forest Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.91      0.94      0.93       101\n",
      "entertainment       1.00      0.91      0.95        81\n",
      "     politics       0.94      0.94      0.94        83\n",
      "        sport       0.99      0.98      0.98        98\n",
      "         tech       0.93      0.99      0.96        82\n",
      "\n",
      "     accuracy                           0.95       445\n",
      "    macro avg       0.95      0.95      0.95       445\n",
      " weighted avg       0.95      0.95      0.95       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, category, test_size=0.2, random_state=42)  #Split data\n",
    "\n",
    "#Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)  #Make predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)  #Calculate accuracy\n",
    "    report = classification_report(y_test, predictions)  #Generate classification report\n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear', class_weight='balanced')  #Initialize SVM model with linear kernel and class weights\n",
    "svm_model.fit(X_train, y_train)  #Train the model\n",
    "svm_accuracy, svm_report = evaluate_model(svm_model, X_test, y_test)  #Evaluate SVM\n",
    "print(\"SVM Accuracy:\", svm_accuracy)  #Print accuracy\n",
    "print(\"SVM Classification Report:\\n\", svm_report)  #Print classification report\n",
    "\n",
    "\n",
    "nb_model = MultinomialNB()  #Initialize Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)  #Train the model\n",
    "nb_accuracy, nb_report = evaluate_model(nb_model, X_test, y_test)  #Evaluate Naive Bayes\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)  #Print accuracy\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)  #Print classification report\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)  #Initialize Random Forest model with class weights\n",
    "rf_model.fit(X_train, y_train)  #Train the model\n",
    "rf_accuracy, rf_report = evaluate_model(rf_model, X_test, y_test)  #Evaluate Random Forest\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)  #Print accuracy\n",
    "print(\"Random Forest Classification Report:\\n\", rf_report)  #Print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02982ee9-b352-4e9f-8bf4-cdae855043c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category is: tech\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained model and TF-IDF vectorizer\n",
    "with open(\"model_bbc.pickle\", \"rb\") as model_file:\n",
    "    lg_model = pickle.load(model_file)\n",
    "\n",
    "with open(\"tfidf_file.pickle\", \"rb\") as tfidf_file:\n",
    "    tfidf_vectorizer = pickle.load(tfidf_file)\n",
    "\n",
    "\n",
    "def predict_category(text):\n",
    "    cleaned_text = preprocess_data(text)\n",
    "    tfidf_data = tfidf_vectorizer.transform([cleaned_text])\n",
    "    prediction = lg_model.predict(tfidf_data)\n",
    "    \n",
    "    category_map = {0: 'politics', 1: 'sport', 2: 'tech', 3: 'entertainment', 4: 'business'}\n",
    "    return category_map[prediction[0]]\n",
    "\n",
    "new_text = \"video games \"\n",
    "predicted_category = predict_category(new_text)\n",
    "print(f\"The predicted category is: {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "86fbfcc2-5fe7-429a-b2f8-c50309aa41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model \n",
    "#Load the trained model and TF-IDF vectorizer from pickle files\n",
    "with open(\"model_bbc.pickle\", \"rb\") as model_file:\n",
    "    lg_model = pickle.load(model_file)  #Load the logistic regression model\n",
    "\n",
    "with open(\"tfidf_file.pickle\", \"rb\") as tfidf_file:\n",
    "    tfidf_vectorizer = pickle.load(tfidf_file)  #Load the TF-IDF vectorizer\n",
    "\n",
    "#Function to predict the category of the input text\n",
    "def predict_category(text):\n",
    "    #Preprocess the input text\n",
    "    cleaned_text = preprocess_data(text)\n",
    "    #Convert the cleaned text to TF-IDF features\n",
    "    tfidf_data = tfidf_vectorizer.transform([cleaned_text])\n",
    "    #Make the prediction using the loaded model\n",
    "    prediction = lg_model.predict(tfidf_data)\n",
    "    \n",
    "    #Map numerical predictions to category names\n",
    "    category_map = {0: 'politics', 1: 'sport', 2: 'tech', 3: 'entertainment', 4: 'business'}\n",
    "    #Return the predicted category\n",
    "    return category_map[prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2133e110-a4e0-45d7-aa48-cc2999b22c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category is: politics\n"
     ]
    }
   ],
   "source": [
    "#Example usage of the prediction function\n",
    "new_text = \"how political squabbles snowball it s become commonplace to argue that blair and brown are like squabbling school kids and that they (and their supporters) need to grow up and stop bickering.  but this analysis in fact gets it wrong. it s not just children who fight - adults do too. and there are solid reasons why even a trivial argument between mature protagonists can be hard to stop once its got going. the key feature of an endless feud is that everyone can agree they d be better off if it ended - but everyone wants to have the last word.  each participant genuinely wants the row to stop  but thinks it worth prolonging the argument just a tiny bit to ensure their view is heard. their successive attempts to end the argument with their last word ensure the argument goes on and on and on. (in the case of mr blair and mr brown  successive books are published  ensuring the issues never die.) now this isn t because the participants are stupid - it s actually each individual behaving entirely rationally  given the incentives facing them. indeed  there s even a piece of economic theory that explains all this. nothing as obscure as  post-neo-classical endogenous growth theory  which the chancellor himself once quoted - but a ubiquitous piece of game theory which all respectable policy wonks are familiar with.  it s often referred to as the  prisoner s dilemma   based on a parable much told in economics degree courses... about a sheriff and two prisoners. the story goes that two prisoners are jointly charged with a heinous crime  and are locked up in separate cells. but the sheriff desperately needs a confession from at least one of them  to provide enough evidence to convict them of the crime. without a confession  the prisoners will get a minimal sentence on some trumped up charge.  clearly the prisoners  best strategy is to keep their mouths shut  and take the short sentence  but the clever sheriff has an idea to induce them to talk. he tells each prisoner separately  that if they confess - and they are the only one to confess - they ll be let off their crime. and he tells them that if they don t confess - and they are the only one not to confess - they ll get life. now  if you are prisoner confronted with this choice  your best bet is to confess. if your partner doesn t confess  you ll get off completely. and if your partner does confess  you d better confess to ensure you don t get life. the result is of course  both prisoners confess  so the sheriff does not have to let either one off. both prisoners  individual logic was to behave that way  even though both would have been better if they had somehow agreed to shut up. don t worry if you don t entirely follow it - you can to look it up on google  where there are 283 000 entries on it.  the prisoners  dilemma and all its ramifications have truly captured economists in the last couple of decades. it is a parable used to describe any situation where there is an obvious sensible choice to be taken collectively  but where the only rational choice individually is to behave selfishly.  a cold war arms race for example - a classic case where both russia and america would be better off with just a few arms  rather than a lot of arms. but as long as each wants just a few more arms than the other  an arms race ensues with the results that the individually logical decision to buy more arms  results in arms levels that are too high. what economics tells us is that once you re in a prisoners  dilemma - unless you are repeating the experience many times over - it s hard to escape the perverse logic of it. it s no good just exhorting people to stop buying arms  or to stop arguing when all their incentives encourage them to carry on. somehow  the incentives have to change.  in the case of the labour party  if you believe the rift between blair and brown camps is as bad as the reports suggest  solomon s wisdom needs to be deployed to solve the problem. every parent knows there are ingenious solutions to arguments  solutions which affect the incentives of the participants. an example  is the famous rule that  one divides  the other chooses  as a way of allocating a piece of cake to be sliced up between greedy children. in the case of an apparently endless argument  if you want it to come to an end  you have to ensure the person who has the last word is one who loses rather than the one who wins the row. the cost of prolonging the row by even one more briefing  or one more book for that matter  has to exceed the benefit of having the last word  and getting your point in. if the rest of the party can enforce that  they ll have the protagonists retreating pretty quickly.\"\n",
    " \n",
    "#Input text for prediction\n",
    "predicted_category = predict_category(new_text)  #Get the predicted category\n",
    "#Output the predicted category\n",
    "print(f\"The predicted category is: {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8018c8-56cb-45dd-aed7-acc17c7d604e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
